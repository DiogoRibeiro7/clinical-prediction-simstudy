# =======================================================================
# replicate_framework.R
# 
# Enhanced implementation of Lohmann et al. (2023) simulation framework
# with comprehensive error handling, logging, configuration management,
# and extensible method registry
#
# Key improvements:
# - Centralized configuration management
# - Structured logging system
# - Memory-efficient processing
# - Comprehensive error handling
# - Method registry for extensibility
# - Performance profiling capabilities
# - Automated validation
#
# Author: Diogo Ribeiro (Enhanced)
# Date: January 2025
# License: MIT
# =======================================================================

# Load required packages with enhanced error handling
suppressPackageStartupMessages({
  required_packages <- c(
    # Core packages
    "MASS", "glmnet", "pROC", "plsRglm", "Matrix",
    # Data manipulation
    "dplyr", "purrr", "tibble", "tidyr", "readr",
    # Configuration and logging
    "yaml", "logger",
    # Performance and memory
    "profvis", "bench", "pryr",
    # Validation
    "testthat", "checkmate"
  )
  
  missing_packages <- required_packages[!sapply(required_packages, requireNamespace, quietly = TRUE)]
  if (length(missing_packages) > 0) {
    warning(paste("Missing optional packages:", paste(missing_packages, collapse = ", ")))
    cat("Install missing packages with:\n")
    cat(paste("install.packages(c(", paste0('"', missing_packages, '"', collapse = ", "), "))\n"))
  }
  
  # Load essential packages
  essential_packages <- c("MASS", "glmnet", "pROC", "plsRglm", "Matrix", 
                         "dplyr", "purrr", "tibble", "tidyr")
  
  for (pkg in essential_packages) {
    if (!requireNamespace(pkg, quietly = TRUE)) {
      stop(paste("Essential package", pkg, "is required but not available"))
    }
    library(pkg, character.only = TRUE)
  }
  
  # Load optional packages silently
  optional_packages <- setdiff(required_packages, essential_packages)
  for (pkg in optional_packages) {
    if (requireNamespace(pkg, quietly = TRUE)) {
      library(pkg, character.only = TRUE)
    }
  }
})

# =======================================================================
# CONFIGURATION MANAGEMENT
# =======================================================================

#' Global configuration object
.simulation_config <- NULL

#' Load configuration from YAML file
#' 
#' @param config_path Path to configuration YAML file
#' @return List containing configuration parameters
load_config <- function(config_path = "config/simulation_config.yaml") {
  if (!file.exists(config_path)) {
    warning(paste("Configuration file not found:", config_path, "- using defaults"))
    return(get_default_config())
  }
  
  tryCatch({
    config <- yaml::read_yaml(config_path)
    .simulation_config <<- config
    cat("âœ… Configuration loaded from:", config_path, "\n")
    return(config)
  }, error = function(e) {
    warning(paste("Failed to load configuration:", e$message, "- using defaults"))
    return(get_default_config())
  })
}

#' Get default configuration (fallback)
get_default_config <- function() {
  list(
    simulation = list(
      default_iterations = 20,
      cv_folds = 10,
      max_pls_components = 30,
      validation_scale = 20,
      base_seed = 20250923L
    ),
    numerical = list(
      calibration_bounds = c(0.01, 10.0),
      eigenvalue_threshold = 1e-8,
      convergence_tolerance = 1e-10,
      probability_bounds = c(1e-12, 1 - 1e-12)
    ),
    parallel = list(
      default_workers = parallel::detectCores() - 1,
      chunk_size = 50,
      memory_limit_gb = 8
    ),
    output = list(
      precision = list(auc = 3, brier = 3, cal_slope = 2, rmspe = 3, mape = 3),
      directories = list(results = "results", logs = "logs", plots = "plots")
    ),
    logging = list(
      level = "INFO",
      console = TRUE,
      file = FALSE
    ),
    methods = list(
      glmnet = list(nlambda = 100, standardize = FALSE),
      pcr = list(min_components = 2)
    )
  )
}

#' Get configuration value with fallback
#' 
#' @param key_path Vector of keys to traverse (e.g., c("simulation", "cv_folds"))
#' @param default Default value if key not found
get_config <- function(key_path, default = NULL) {
  if (is.null(.simulation_config)) {
    .simulation_config <<- get_default_config()
  }
  
  value <- .simulation_config
  for (key in key_path) {
    if (is.list(value) && key %in% names(value)) {
      value <- value[[key]]
    } else {
      return(default)
    }
  }
  value
}

# Initialize configuration
if (file.exists("config/simulation_config.yaml")) {
  load_config()
} else {
  .simulation_config <- get_default_config()
}

# =======================================================================
# LOGGING SYSTEM
# =======================================================================

#' Initialize logging system
#' 
#' @param level Logging level (DEBUG, INFO, WARN, ERROR)
#' @param log_file Optional file path for logging
#' @param console Whether to log to console
init_logging <- function(level = NULL, log_file = NULL, console = NULL) {
  if (is.null(level)) level <- get_config(c("logging", "level"), "INFO")
  if (is.null(console)) console <- get_config(c("logging", "console"), TRUE)
  if (is.null(log_file) && get_config(c("logging", "file"), FALSE)) {
    log_dir <- get_config(c("output", "directories", "logs"), "logs")
    dir.create(log_dir, showWarnings = FALSE, recursive = TRUE)
    log_file <- file.path(log_dir, paste0("simulation_", format(Sys.time(), "%Y%m%d_%H%M%S"), ".log"))
  }
  
  if (requireNamespace("logger", quietly = TRUE)) {
    logger::log_threshold(level)
    
    if (console) {
      logger::log_appender(logger::appender_console)
    }
    
    if (!is.null(log_file)) {
      logger::log_appender(logger::appender_file(log_file))
      cat("ðŸ“ Logging to file:", log_file, "\n")
    }
  } else {
    # Fallback to cat-based logging
    warning("logger package not available - using basic logging")
  }
}

#' Enhanced logging wrapper
log_info <- function(msg, ...) {
  formatted_msg <- sprintf(msg, ...)
  if (requireNamespace("logger", quietly = TRUE)) {
    logger::log_info(formatted_msg)
  } else {
    cat("[INFO]", formatted_msg, "\n")
  }
}

log_warn <- function(msg, ...) {
  formatted_msg <- sprintf(msg, ...)
  if (requireNamespace("logger", quietly = TRUE)) {
    logger::log_warn(formatted_msg)
  } else {
    cat("[WARN]", formatted_msg, "\n")
  }
}

log_error <- function(msg, ...) {
  formatted_msg <- sprintf(msg, ...)
  if (requireNamespace("logger", quietly = TRUE)) {
    logger::log_error(formatted_msg)
  } else {
    cat("[ERROR]", formatted_msg, "\n")
  }
}

log_debug <- function(msg, ...) {
  formatted_msg <- sprintf(msg, ...)
  if (requireNamespace("logger", quietly = TRUE)) {
    logger::log_debug(formatted_msg)
  }
  # No fallback for debug messages
}

# Initialize logging
init_logging()

# =======================================================================
# ENHANCED UTILITY FUNCTIONS
# =======================================================================

#' Enhanced clip function with validation
#' 
#' @param x Numeric vector to clip
#' @param lo Lower bound
#' @param hi Upper bound
#' @return Clipped numeric vector
clip <- function(x, lo = -Inf, hi = Inf) {
  checkmate::assert_numeric(x)
  checkmate::assert_number(lo)
  checkmate::assert_number(hi)
  
  if (lo > hi) {
    stop("Lower bound cannot exceed upper bound")
  }
  
  result <- pmin(pmax(x, lo), hi)
  
  n_clipped <- sum(x < lo | x > hi, na.rm = TRUE)
  if (n_clipped > 0) {
    log_debug("Clipped %d values to range [%.3f, %.3f]", n_clipped, lo, hi)
  }
  
  result
}

#' Enhanced logit transformation
#' 
#' @param p Probability vector
#' @param eps Small constant to prevent log(0)
#' @return Log-odds vector
logit <- function(p, eps = NULL) {
  if (is.null(eps)) {
    eps <- get_config(c("numerical", "probability_bounds"), c(1e-12, 1-1e-12))[1]
  }
  
  checkmate::assert_numeric(p, lower = 0, upper = 1, any.missing = FALSE)
  
  # Enhanced bounds based on configuration
  bounds <- get_config(c("numerical", "probability_bounds"), c(eps, 1 - eps))
  p_clipped <- clip(p, bounds[1], bounds[2])
  
  log(p_clipped / (1 - p_clipped))
}

#' Enhanced calibration slope winsorization
#' 
#' @param b Calibration slope estimate
#' @param bounds Vector of [lower, upper] bounds
#' @return Winsorized slope value
winsorize_cal_slope <- function(b, bounds = NULL) {
  if (is.null(bounds)) {
    bounds <- get_config(c("numerical", "calibration_bounds"), c(0.01, 10))
  }
  
  checkmate::assert_numeric(b, len = 1)
  checkmate::assert_numeric(bounds, len = 2)
  
  clip(b, bounds[1], bounds[2])
}

# =======================================================================
# MEMORY MANAGEMENT
# =======================================================================

#' Estimate memory usage for a scenario
#' 
#' @param scenario Single scenario from grid
#' @return Estimated memory usage in bytes
estimate_scenario_memory <- function(scenario) {
  # Rough estimation based on data dimensions
  n_train <- max(200, ceiling((scenario$EPV * scenario$p) / scenario$event_frac))
  n_val <- max(200, ceiling(20 * scenario$p / scenario$event_frac))
  
  # Memory per matrix element (8 bytes for numeric)
  memory_per_element <- 8
  
  # Training and validation matrices + results
  matrix_memory <- (n_train + n_val) * scenario$p * memory_per_element
  
  # Additional overhead for intermediate calculations (factor of 3)
  total_memory <- matrix_memory * 3
  
  total_memory
}

#' Check available memory and suggest batch size
#' 
#' @param scenarios Vector of scenario indices
#' @param grid Complete scenario grid
#' @param max_memory_gb Maximum memory to use
#' @return Suggested batch size
suggest_batch_size <- function(scenarios, grid, max_memory_gb = NULL) {
  if (is.null(max_memory_gb)) {
    max_memory_gb <- get_config(c("parallel", "memory_limit_gb"), 8)
  }
  
  if (length(scenarios) == 0) return(1)
  
  # Estimate memory for first scenario (representative)
  sample_scenario <- grid[scenarios[1], ]
  memory_per_scenario <- estimate_scenario_memory(sample_scenario)
  
  # Conservative batch size calculation
  max_memory_bytes <- max_memory_gb * 1e9
  suggested_batch <- floor(max_memory_bytes / (memory_per_scenario * 2))
  
  # Ensure reasonable bounds
  suggested_batch <- max(1, min(suggested_batch, length(scenarios), 100))
  
  log_info("Suggested batch size: %d scenarios (%.1f MB estimated per scenario)", 
           suggested_batch, memory_per_scenario / 1e6)
  
  suggested_batch
}

# =======================================================================
# ENHANCED DATA GENERATION
# =======================================================================

#' Enhanced correlation matrix generation with validation
#' 
#' @param p Number of predictors
#' @param beta_a Beta distribution shape parameter 1
#' @param beta_b Beta distribution shape parameter 2
#' @param min_eigenvalue Minimum eigenvalue for positive definiteness
#' @return Validated correlation matrix
make_sigma <- function(p, beta_a = 1, beta_b = 3, min_eigenvalue = NULL) {
  checkmate::assert_int(p, lower = 1)
  checkmate::assert_number(beta_a, lower = 0)
  checkmate::assert_number(beta_b, lower = 0)
  
  if (is.null(min_eigenvalue)) {
    min_eigenvalue <- get_config(c("numerical", "eigenvalue_threshold"), 1e-8)
  }
  
  log_debug("Generating %dx%d correlation matrix", p, p)
  
  # Generate correlations
  n_off_diag <- p * (p - 1) / 2
  r_abs <- rbeta(n_off_diag, beta_a, beta_b)
  signs <- sample(c(-1, 1), n_off_diag, replace = TRUE)
  r <- r_abs * signs
  
  # Fill correlation matrix
  Sigma <- diag(1, p)
  k <- 1L
  for (i in seq_len(p - 1L)) {
    for (j in (i + 1L):p) {
      Sigma[i, j] <- r[k]
      Sigma[j, i] <- r[k]
      k <- k + 1L
    }
  }
  
  # Enhanced eigenvalue correction
  eig <- eigen(Sigma, symmetric = TRUE)
  
  if (min(eig$values) < min_eigenvalue) {
    log_debug("Correcting eigenvalues: min=%.2e, threshold=%.2e", 
              min(eig$values), min_eigenvalue)
    
    correction_factor <- get_config(c("numerical", "eigenvalue_correction_factor"), 0.001)
    eig$values[eig$values < min_eigenvalue] <- min_eigenvalue + correction_factor * abs(min(eig$values))
    
    # Reconstruct matrix
    Sigma <- eig$vectors %*% diag(eig$values, p) %*% t(eig$vectors)
    
    # Re-standardize to correlation matrix
    D <- diag(1 / sqrt(diag(Sigma)), p)
    Sigma <- D %*% Sigma %*% D
  }
  
  # Final validation
  if (!isSymmetric(Sigma, tol = 1e-10)) {
    log_warn("Correlation matrix not perfectly symmetric - correcting")
    Sigma <- (Sigma + t(Sigma)) / 2
  }
  
  # Ensure diagonal is exactly 1
  diag(Sigma) <- 1
  
  log_debug("Final correlation matrix: eigenvalue range [%.2e, %.2e]", 
            min(eigen(Sigma)$values), max(eigen(Sigma)$values))
  
  as.matrix(Sigma)
}

#' Enhanced intercept solving with better convergence
#' 
#' @param beta Coefficient vector
#' @param X Predictor matrix  
#' @param target_pi Target event probability
#' @param tol Convergence tolerance
#' @param max_iter Maximum iterations
#' @return Intercept value
solve_intercept <- function(beta, X, target_pi, tol = NULL, max_iter = 100) {
  if (is.null(tol)) {
    tol <- get_config(c("numerical", "convergence_tolerance"), 1e-10)
  }
  
  checkmate::assert_numeric(beta, len = ncol(X))
  checkmate::assert_matrix(X, mode = "numeric")
  checkmate::assert_number(target_pi, lower = 0, upper = 1)
  
  # Define objective function
  f <- function(b0) {
    mean(plogis(b0 + as.numeric(X %*% beta))) - target_pi
  }
  
  # Try progressively wider bounds with enhanced error handling
  bounds_list <- list(
    c(-10, 10),
    c(-20, 20),
    c(-50, 50),
    c(-100, 100)
  )
  
  for (bounds in bounds_list) {
    tryCatch({
      result <- uniroot(f, bounds, tol = tol, maxiter = max_iter)
      log_debug("Intercept solved: b0=%.6f, bounds=[%.1f, %.1f]", 
                result$root, bounds[1], bounds[2])
      return(result$root)
    }, error = function(e) {
      log_debug("Intercept solving failed with bounds [%.1f, %.1f]: %s", 
                bounds[1], bounds[2], e$message)
    })
  }
  
  # Final fallback with relaxed tolerance
  tryCatch({
    result <- uniroot(f, c(-200, 200), tol = tol * 100, maxiter = max_iter * 2)
    log_warn("Intercept solved with relaxed tolerance: b0=%.6f", result$root)
    return(result$root)
  }, error = function(e) {
    log_error("All intercept solving methods failed: %s", e$message)
    # Return logit approximation as last resort
    fallback <- qlogis(target_pi)
    log_warn("Using logit approximation: b0=%.6f", fallback)
    return(fallback)
  })
}

#' Enhanced dataset generation with comprehensive validation
#' 
#' @param p Number of predictors
#' @param event_frac Target event probability
#' @param EPV Events per variable
#' @param noise_frac Fraction of noise predictors  
#' @param sparse Whether to include binary predictors
#' @param seed Random seed
#' @param n_val_scale Validation set size multiplier
#' @return Complete dataset with metadata
gen_dataset <- function(p, event_frac, EPV, noise_frac, sparse, seed,
                        n_val_scale = NULL) {
  
  # Input validation
  checkmate::assert_int(p, lower = 1)
  checkmate::assert_number(event_frac, lower = 0, upper = 1)
  checkmate::assert_number(EPV, lower = 1)
  checkmate::assert_number(noise_frac, lower = 0, upper = 1)
  checkmate::assert_logical(sparse, len = 1)
  checkmate::assert_int(seed)
  
  if (is.null(n_val_scale)) {
    n_val_scale <- get_config(c("simulation", "validation_scale"), 20)
  }
  
  set.seed(seed)
  log_debug("Generating dataset: p=%d, event_frac=%.3f, EPV=%d, noise_frac=%.2f, sparse=%s, seed=%d",
            p, event_frac, EPV, noise_frac, sparse, seed)
  
  # Calculate sample sizes
  n_train <- max(200L, ceiling((EPV * p) / event_frac))
  n_val <- max(200L, ceiling(n_val_scale * p / event_frac))
  
  log_debug("Sample sizes: n_train=%d, n_val=%d", n_train, n_val)
  
  # Generate data with enhanced error handling
  tryCatch({
    # Generate correlation matrix and data
    Sigma <- make_sigma(p)
    Xtr <- MASS::mvrnorm(n_train, rep(0, p), Sigma)
    Xva <- MASS::mvrnorm(n_val, rep(0, p), Sigma)
    
    # Standardization
    Xtr_s <- scale(Xtr, center = TRUE, scale = TRUE)
    scale_center <- attr(Xtr_s, "scaled:center")
    scale_scale <- attr(Xtr_s, "scaled:scale")
    
    # Handle potential scaling issues
    if (any(scale_scale == 0)) {
      log_warn("Zero variance predictors detected - adding small noise")
      scale_scale[scale_scale == 0] <- 1
    }
    
    Xva_s <- scale(Xva, center = scale_center, scale = scale_scale)
    
    # Apply sparse transformation
    sparse_indices <- NULL
    if (isTRUE(sparse)) {
      n_sparse <- max(1L, floor(p / 4))
      sparse_indices <- sample(seq_len(p), n_sparse)
      
      Xtr_s[, sparse_indices] <- (Xtr_s[, sparse_indices] > 0) * 1
      Xva_s[, sparse_indices] <- (Xva_s[, sparse_indices] > 0) * 1
      
      log_debug("Applied sparse transformation to %d predictors", n_sparse)
    }
    
    # Generate coefficients and outcomes
    beta_true <- draw_true_beta(p, noise_frac)
    b0_true <- solve_intercept(beta_true, Xtr_s, target_pi = event_frac)
    
    # Generate probabilities and outcomes
    pi_tr <- plogis(b0_true + as.numeric(Xtr_s %*% beta_true))
    pi_va <- plogis(b0_true + as.numeric(Xva_s %*% beta_true))
    
    y_tr <- rbinom(n_train, 1, pi_tr)
    y_va <- rbinom(n_val, 1, pi_va)
    
    # Validation checks
    actual_event_rate_tr <- mean(y_tr)
    actual_event_rate_va <- mean(y_va)
    
    log_debug("Actual event rates: train=%.3f (target=%.3f), val=%.3f", 
              actual_event_rate_tr, event_frac, actual_event_rate_va)
    
    if (abs(actual_event_rate_tr - event_frac) > 0.1) {
      log_warn("Training event rate (%.3f) deviates from target (%.3f)",
               actual_event_rate_tr, event_frac)
    }
    
    # Return comprehensive dataset
    list(
      # Data matrices
      Xtr = Xtr_s,
      ytr = y_tr, 
      Xva = Xva_s,
      yva = y_va,
      piv = pi_va,
      
      # True parameters
      beta_true = beta_true,
      b0_true = b0_true,
      
      # Preprocessing metadata
      scale_center = scale_center,
      scale_scale = scale_scale,
      sparse_indices = sparse_indices,
      
      # Sample information
      n_train = n_train,
      n_val = n_val,
      actual_event_rate_tr = actual_event_rate_tr,
      actual_event_rate_va = actual_event_rate_va,
      
      # Generation metadata
      seed = seed,
      correlation_matrix = Sigma
    )
    
  }, error = function(e) {
    log_error("Dataset generation failed: %s", e$message)
    stop(paste("Dataset generation failed:", e$message))
  })
}

#' Enhanced coefficient generation
draw_true_beta <- function(p, noise_frac = 0) {
  checkmate::assert_int(p, lower = 1)
  checkmate::assert_number(noise_frac, lower = 0, upper = 1)
  
  beta <- rnorm(p, 0, 1)
  
  if (noise_frac > 0) {
    k0 <- floor(noise_frac * p)
    if (k0 > 0) {
      noise_indices <- sample(seq_len(p), k0)
      beta[noise_indices] <- 0
      log_debug("Set %d/%d coefficients to zero (noise_frac=%.2f)", k0, p, noise_frac)
    }
  }
  
  beta
}

# =======================================================================
# ENHANCED PERFORMANCE EVALUATION
# =======================================================================

#' Comprehensive performance metrics with enhanced error handling
#' 
#' @param phat Predicted probabilities
#' @param y Observed outcomes
#' @param pi_true True probabilities
#' @return Tibble with all metrics and quality flags
compute_metrics <- function(phat, y, pi_true) {
  
  # Input validation
  checkmate::assert_numeric(phat, lower = 0, upper = 1, any.missing = FALSE)
  checkmate::assert_integerish(y, lower = 0, upper = 1, any.missing = FALSE)
  checkmate::assert_numeric(pi_true, lower = 0, upper = 1, any.missing = FALSE)
  checkmate::assert_true(length(phat) == length(y))
  checkmate::assert_true(length(phat) == length(pi_true))
  
  log_debug("Computing metrics for %d predictions", length(phat))
  
  # Initialize results with quality flags
  results <- list(
    auc = NA_real_,
    cal_slope = NA_real_,
    cal_in_large = NA_real_,
    brier = NA_real_,
    rMSPE = NA_real_,
    MAPE = NA_real_,
    quality_flags = list()
  )
  
  # 1. AUC calculation with enhanced validation
  tryCatch({
    if (length(unique(y)) < 2) {
      results$quality_flags <- append(results$quality_flags, "no_outcome_variation")
      log_debug("Cannot compute AUC: no outcome variation")
    } else if (length(unique(phat)) < 2) {
      results$quality_flags <- append(results$quality_flags, "no_prediction_variation") 
      log_debug("Cannot compute AUC: no prediction variation")
    } else {
      auc_obj <- pROC::roc(y, phat, quiet = TRUE, direction = "<")
      results$auc <- as.numeric(auc_obj$auc)
      
      # Quality checks
      if (results$auc < 0.5) {
        results$quality_flags <- append(results$quality_flags, "auc_below_chance")
        log_debug("AUC below chance level: %.3f", results$auc)
      }
    }
  }, error = function(e) {
    results$quality_flags <<- append(results$quality_flags, "auc_computation_error")
    log_warn("AUC computation failed: %s", e$message)
  })
  
  # 2. Calibration slope with enhanced numerical stability
  tryCatch({
    lph <- logit(phat)
    
    if (any(is.infinite(lph)) || any(is.na(lph))) {
      results$quality_flags <- append(results$quality_flags, "invalid_logit_predictions")
      log_debug("Invalid logit predictions detected")
    } else if (length(unique(y)) < 2) {
      results$quality_flags <- append(results$quality_flags, "calibration_no_variation")
    } else {
      # Robust GLM fitting
      cal_data <- data.frame(y = y, lph = lph)
      cal_fit <- glm(y ~ lph, family = binomial(), data = cal_data,
                     control = glm.control(maxit = 100, epsilon = 1e-8))
      
      if (cal_fit$converged && !any(is.na(coef(cal_fit)))) {
        raw_slope <- coef(cal_fit)[2]
        results$cal_slope <- winsorize_cal_slope(raw_slope)
        
        # Quality flags for calibration
        if (abs(raw_slope - results$cal_slope) > 0.01) {
          results$quality_flags <- append(results$quality_flags, "calibration_slope_winsorized")
          log_debug("Calibration slope winsorized: %.3f -> %.3f", raw_slope, results$cal_slope)
        }
        
        if (abs(results$cal_slope - 1) > 2) {
          results$quality_flags <- append(results$quality_flags, "severe_miscalibration")
        }
      } else {
        results$quality_flags <- append(results$quality_flags, "calibration_fit_failed")
        log_debug("Calibration GLM did not converge or has invalid coefficients")
      }
    }
  }, error = function(e) {
    results$quality_flags <<- append(results$quality_flags, "calibration_computation_error")
    log_warn("Calibration slope computation failed: %s", e$message)
  })
  
  # 3. Calibration-in-the-large with enhanced stability
  tryCatch({
    if (exists("lph") && !any(is.infinite(lph)) && !any(is.na(lph)) && length(unique(y)) >= 2) {
      cil_data <- data.frame(y = y)
      cil_fit <- glm(y ~ 1, family = binomial(), data = cil_data, offset = lph,
                     control = glm.control(maxit = 100, epsilon = 1e-8))
      
      if (cil_fit$converged && !any(is.na(coef(cil_fit)))) {
        results$cal_in_large <- coef(cil_fit)[1]
        
        # Quality flag for extreme miscalibration
        if (abs(results$cal_in_large) > 5) {
          results$quality_flags <- append(results$quality_flags, "extreme_calibration_in_large")
        }
      } else {
        results$quality_flags <- append(results$quality_flags, "cil_fit_failed")
      }
    }
  }, error = function(e) {
    results$quality_flags <<- append(results$quality_flags, "cil_computation_error")
    log_warn("Calibration-in-the-large computation failed: %s", e$message)
  })
  
  # 4. Brier score (always computable)
  results$brier <- mean((y - phat)^2)
  
  # Quality check for Brier score
  if (results$brier > 0.5) {
    results$quality_flags <- append(results$quality_flags, "high_brier_score")
    log_debug("High Brier score detected: %.4f", results$brier)
  }
  
  # 5. rMSPE (root mean squared prediction error vs true probabilities)
  results$rMSPE <- sqrt(mean((pi_true - phat)^2))
  
  # 6. MAPE (mean absolute prediction error vs true probabilities)
  results$MAPE <- mean(abs(pi_true - phat))
  
  # Overall quality assessment
  n_flags <- length(results$quality_flags)
  if (n_flags > 0) {
    log_debug("Quality flags (%d): %s", n_flags, paste(results$quality_flags, collapse = ", "))
  }
  
  # Return as tibble (excluding quality_flags from main result)
  tibble(
    auc = results$auc,
    cal_slope = results$cal_slope,
    cal_in_large = results$cal_in_large, 
    brier = results$brier,
    rMSPE = results$rMSPE,
    MAPE = results$MAPE,
    n_quality_flags = n_flags
  )
}

# =======================================================================
# METHOD REGISTRY SYSTEM
# =======================================================================

#' Global method registry
.method_registry <- list()

#' Register a new method in the framework
#' 
#' @param method_name Unique method identifier
#' @param fit_func Function to fit the method (signature: fit_func(X, y, foldid, ...))
#' @param predict_func Function to make predictions (signature: predict_func(fit, X))
#' @param requires_cv Whether method requires cross-validation folds
#' @param fallback_method Method to use if this method fails
#' @param description Human-readable description
register_method <- function(method_name, fit_func, predict_func, 
                           requires_cv = TRUE, fallback_method = "MLE", 
                           description = "") {
  
  checkmate::assert_string(method_name)
  checkmate::assert_function(fit_func)
  checkmate::assert_function(predict_func)
  checkmate::assert_logical(requires_cv, len = 1)
  checkmate::assert_string(fallback_method)
  
  # Validate function signatures
  fit_args <- names(formals(fit_func))
  predict_args <- names(formals(predict_func))
  
  required_fit_args <- c("X", "y")
  required_predict_args <- c("fit", "X")
  
  if (!all(required_fit_args %in% fit_args)) {
    stop(paste("fit_func must have arguments:", paste(required_fit_args, collapse = ", ")))
  }
  
  if (!all(required_predict_args %in% predict_args)) {
    stop(paste("predict_func must have arguments:", paste(required_predict_args, collapse = ", ")))
  }
  
  .method_registry[[method_name]] <<- list(
    fit = fit_func,
    predict = predict_func,
    requires_cv = requires_cv,
    fallback = fallback_method,
    description = description,
    registered_at = Sys.time()
  )
  
  log_info("Registered method: %s (%s)", method_name, description)
}

#' Get registered method
get_method <- function(method_name) {
  if (method_name %in% names(.method_registry)) {
    return(.method_registry[[method_name]])
  } else {
    stop(paste("Method not found in registry:", method_name))
  }
}

#' List all registered methods
list_methods <- function() {
  if (length(.method_registry) == 0) {
    return("No methods registered")
  }
  
  methods_df <- data.frame(
    method = names(.method_registry),
    requires_cv = sapply(.method_registry, function(x) x$requires_cv),
    fallback = sapply(.method_registry, function(x) x$fallback),
    description = sapply(.method_registry, function(x) x$description),
    stringsAsFactors = FALSE
  )
  
  methods_df
}

# =======================================================================
# ENHANCED CROSS-VALIDATION
# =======================================================================

#' Enhanced cross-validation fold creation
#' 
#' @param n Sample size
#' @param K Number of folds
#' @param seed Random seed
#' @param stratify Optional binary outcome for stratified folds
#' @return Vector of fold assignments
make_folds <- function(n, K = NULL, seed = 1, stratify = NULL) {
  if (is.null(K)) {
    K <- get_config(c("simulation", "cv_folds"), 10)
  }
  
  checkmate::assert_int(n, lower = 1)
  checkmate::assert_int(K, lower = 2)
  checkmate::assert_int(seed)
  
  set.seed(seed)
  
  # Handle edge cases
  if (n < K) {
    log_warn("Fewer observations (%d) than folds (%d) - using leave-one-out", n, K)
    return(seq_len(n))
  }
  
  if (!is.null(stratify)) {
    # Stratified CV for binary outcomes
    checkmate::assert_integerish(stratify, lower = 0, upper = 1, len = n)
    
    if (requireNamespace("caret", quietly = TRUE)) {
      folds <- caret::createFolds(factor(stratify), k = K, list = FALSE)
      return(as.numeric(folds))
    } else {
      log_warn("caret package not available - using simple stratification")
      
      # Simple stratification
      pos_indices <- which(stratify == 1)
      neg_indices <- which(stratify == 0)
      
      folds <- numeric(n)
      if (length(pos_indices) > 0) {
        folds[pos_indices] <- sample(rep(1:K, length.out = length(pos_indices)))
      }
      if (length(neg_indices) > 0) {
        folds[neg_indices] <- sample(rep(1:K, length.out = length(neg_indices)))
      }
      
      return(folds)
    }
  } else {
    # Standard random CV
    return(sample(rep(1:K, length.out = n)))
  }
}

# =======================================================================
# ENHANCED MODEL FITTING FUNCTIONS
# =======================================================================

#' Enhanced MLE fitting with comprehensive diagnostics
#' 
#' @param X Predictor matrix
#' @param y Binary outcome vector
#' @param ... Additional arguments (for consistency with registry)
#' @return Enhanced fit object with diagnostics
fit_mle <- function(X, y, ...) {
  
  checkmate::assert_matrix(X, mode = "numeric")
  checkmate::assert_integerish(y, lower = 0, upper = 1, len = nrow(X))
  
  log_debug("Fitting MLE: n=%d, p=%d", nrow(X), ncol(X))
  
  tryCatch({
    # Check for sufficient variation
    if (length(unique(y)) < 2) {
      stop("No variation in outcome")
    }
    
    # Check for rank deficiency
    if (ncol(X) >= nrow(X)) {
      log_warn("MLE: More predictors than observations (p=%d, n=%d)", ncol(X), nrow(X))
    }
    
    # Prepare data
    df <- as.data.frame(X)
    colnames(df) <- paste0("x", seq_len(ncol(X)))
    df$y <- y
    
    # Enhanced GLM control
    glm_control <- glm.control(
      epsilon = get_config(c("numerical", "convergence_tolerance"), 1e-10),
      maxit = 100,
      trace = FALSE
    )
    
    # Fit model
    start_time <- Sys.time()
    fit <- glm(y ~ ., data = df, family = binomial(), control = glm_control)
    fit_time <- difftime(Sys.time(), start_time, units = "secs")
    
    # Enhanced convergence checking
    if (!fit$converged) {
      log_warn("MLE did not converge after %d iterations", glm_control$maxit)
    }
    
    # Numerical stability checks
    coef_issues <- sum(abs(coef(fit)) > 10, na.rm = TRUE)
    if (coef_issues > 0) {
      log_warn("MLE: %d coefficients with absolute value > 10", coef_issues)
    }
    
    # Add diagnostics to fit object
    fit$diagnostics <- list(
      converged = fit$converged,
      iterations = fit$iter,
      fit_time = as.numeric(fit_time),
      large_coefficients = coef_issues,
      condition_number = NA  # Could add rcond(model.matrix(fit)) if needed
    )
    
    log_debug("MLE fitted in %.3f seconds, converged=%s", fit_time, fit$converged)
    
    fit
    
  }, error = function(e) {
    log_error("MLE fitting failed: %s", e$message)
    
    # Return NULL with diagnostic info
    result <- NULL
    attr(result, "error") <- e$message
    attr(result, "method") <- "MLE"
    result
  })
}

#' Enhanced MLE prediction
#' 
#' @param fit Fitted MLE model
#' @param X New predictor matrix
#' @return Vector of predicted probabilities
predict_mle <- function(fit, X) {
  
  checkmate::assert_matrix(X, mode = "numeric")
  
  if (is.null(fit)) {
    log_debug("MLE prediction with NULL fit - returning default probabilities")
    return(rep(0.5, nrow(X)))
  }
  
  tryCatch({
    # Prepare prediction data
    df <- as.data.frame(X)
    colnames(df) <- names(coef(fit))[-1]  # Exclude intercept name
    
    # Make predictions
    predictions <- predict(fit, df, type = "response")
    
    # Validate predictions
    if (any(is.na(predictions))) {
      n_na <- sum(is.na(predictions))
      log_warn("MLE prediction: %d NA values, replacing with 0.5", n_na)
      predictions[is.na(predictions)] <- 0.5
    }
    
    # Ensure valid probability range
    predictions <- clip(predictions, 1e-15, 1 - 1e-15)
    
    as.numeric(predictions)
    
  }, error = function(e) {
    log_error("MLE prediction failed: %s", e$message)
    rep(0.5, nrow(X))
  })
}

#' Enhanced penalized regression fitting
#' 
#' @param X Predictor matrix
#' @param y Binary outcome vector
#' @param alpha Elastic net mixing parameter
#' @param foldid Cross-validation fold assignments
#' @param relax Whether to use relaxed LASSO
#' @param ... Additional arguments
#' @return Enhanced cv.glmnet object
fit_glmnet_cv <- function(X, y, alpha, foldid = NULL, relax = FALSE, ...) {
  
  checkmate::assert_matrix(X, mode = "numeric")
  checkmate::assert_integerish(y, lower = 0, upper = 1, len = nrow(X))
  checkmate::assert_number(alpha, lower = 0, upper = 1)
  checkmate::assert_logical(relax, len = 1)
  
  method_name <- if (relax) "RelaxedLASSO" else 
                 if (alpha == 0) "Ridge" else 
                 if (alpha == 1) "LASSO" else "ElasticNet"
  
  log_debug("Fitting %s (alpha=%.3f): n=%d, p=%d", method_name, alpha, nrow(X), ncol(X))
  
  tryCatch({
    # Check data validity
    if (length(unique(y)) < 2) {
      stop("No variation in outcome")
    }
    
    # Default fold assignment
    if (is.null(foldid)) {
      foldid <- make_folds(nrow(X), stratify = y, seed = 1)
    }
    
    # Enhanced glmnet parameters
    glmnet_config <- get_config(c("methods", "glmnet"), list())
    
    nlambda <- glmnet_config$nlambda %||% 100
    standardize <- glmnet_config$standardize %||% FALSE
    
    start_time <- Sys.time()
    
    # Fit cross-validated model
    cvfit <- cv.glmnet(
      x = X,
      y = y,
      family = "binomial",
      alpha = alpha,
      relax = relax,
      foldid = foldid,
      nlambda = nlambda,
      type.measure = "deviance",
      standardize = standardize,
      parallel = FALSE,  # We handle parallelization at higher level
      trace.it = 0
    )
    
    fit_time <- difftime(Sys.time(), start_time, units = "secs")
    
    # Add diagnostic information
    cvfit$diagnostics <- list(
      method = method_name,
      alpha = alpha,
      relax = relax,
      fit_time = as.numeric(fit_time),
      n_lambda = length(cvfit$lambda),
      lambda_min = cvfit$lambda.min,
      lambda_1se = cvfit$lambda.1se,
      min_cvm = min(cvfit$cvm),
      nonzero_coef_min = sum(coef(cvfit, s = "lambda.min")[-1] != 0),
      nonzero_coef_1se = sum(coef(cvfit, s = "lambda.1se")[-1] != 0)
    )
    
    log_debug("%s fitted in %.3f seconds, lambda.min=%.2e, nonzero=%d", 
              method_name, fit_time, cvfit$lambda.min, cvfit$diagnostics$nonzero_coef_min)
    
    cvfit
    
  }, error = function(e) {
    log_error("%s fitting failed: %s", method_name, e$message)
    
    result <- NULL
    attr(result, "error") <- e$message
    attr(result, "method") <- method_name
    result
  })
}

#' Enhanced glmnet prediction
#' 
#' @param cvfit cv.glmnet object
#' @param X New predictor matrix
#' @param s Lambda value selection ("lambda.min" or "lambda.1se")
#' @return Vector of predicted probabilities
predict_glmnet_resp <- function(cvfit, X, s = "lambda.min") {
  
  checkmate::assert_matrix(X, mode = "numeric")
  checkmate::assert_choice(s, c("lambda.min", "lambda.1se"))
  
  if (is.null(cvfit)) {
    log_debug("glmnet prediction with NULL fit - returning default probabilities")
    return(rep(0.5, nrow(X)))
  }
  
  method_name <- attr(cvfit, "method") %||% "glmnet"
  
  tryCatch({
    # Make predictions
    predictions <- predict(cvfit, newx = X, type = "response", s = s)
    predictions <- as.numeric(predictions)
    
    # Validate predictions
    if (any(is.na(predictions))) {
      n_na <- sum(is.na(predictions))
      log_warn("%s prediction: %d NA values, replacing with 0.5", method_name, n_na)
      predictions[is.na(predictions)] <- 0.5
    }
    
    # Ensure valid probability range
    predictions <- clip(predictions, 1e-15, 1 - 1e-15)
    
    predictions
    
  }, error = function(e) {
    log_error("%s prediction failed: %s", method_name, e$message)
    rep(0.5, nrow(X))
  })
}

# =======================================================================
# ENHANCED PCR FUNCTIONS
# =======================================================================

#' Enhanced PCR component selection with better validation
#' 
#' @param X Predictor matrix
#' @param y Binary outcome vector
#' @param rule Selection method
#' @param foldid CV fold assignments
#' @return List with PCA object and selected components
select_pcr_components <- function(X, y, rule = c("evgt1", "var90", "aic", "cvdev"), 
                                  foldid = NULL) {
  
  rule <- match.arg(rule)
  
  checkmate::assert_matrix(X, mode = "numeric")
  checkmate::assert_integerish(y, lower = 0, upper = 1, len = nrow(X))
  
  log_debug("PCR component selection (%s): n=%d, p=%d", rule, nrow(X), ncol(X))
  
  tryCatch({
    # Perform PCA with enhanced error handling
    start_time <- Sys.time()
    
    # Check for rank deficiency
    max_rank <- min(nrow(X) - 1, ncol(X))
    if (max_rank <= 0) {
      stop("Insufficient data for PCA")
    }
    
    pr <- prcomp(X, center = FALSE, scale. = FALSE, rank. = max_rank)
    eig <- pr$sdev^2
    varexpl <- cumsum(eig) / sum(eig)
    
    pca_time <- difftime(Sys.time(), start_time, units = "secs")
    log_debug("PCA completed in %.3f seconds, %d components", pca_time, length(eig))
    
    # Component selection bounds
    min_components <- get_config(c("methods", "pcr", "min_components"), 1)
    max_k <- min(ncol(X), nrow(X) - 2, max_rank)
    k_candidates <- seq_len(max_k)
    
    # Apply selection rule with enhanced logic
    k <- switch(
      rule,
      
      # Kaiser criterion: eigenvalue > 1
      evgt1 = {
        k_kaiser <- sum(eig > 1)
        max(min_components, min(k_kaiser, max_k))
      },
      
      # Variance explained: â‰¥90%
      var90 = {
        k90 <- which(varexpl >= 0.90)[1]
        if (is.na(k90)) {
          log_warn("PCR var90: Could not achieve 90%% variance - using max components")
          max_k
        } else {
          max(min_components, k90)
        }
      },
      
      # AIC-based selection with enhanced fitting
      aic = {
        if (length(unique(y)) < 2) {
          min_components
        } else {
          aics <- sapply(k_candidates, function(k) {
            tryCatch({
              Z <- pr$x[, seq_len(k), drop = FALSE]
              fit_data <- data.frame(y = y, Z)
              fit <- glm(y ~ ., family = binomial(), data = fit_data,
                        control = glm.control(maxit = 50))
              if (fit$converged) AIC(fit) else Inf
            }, error = function(e) {
              log_debug("PCR AIC: k=%d failed: %s", k, e$message)
              Inf
            })
          })
          
          if (all(is.infinite(aics))) {
            log_warn("PCR AIC: All models failed - using minimum components")
            min_components
          } else {
            k_candidates[which.min(aics)]
          }
        }
      },
      
      # Cross-validated deviance minimization
      cvdev = {
        if (is.null(foldid) || length(unique(y)) < 2) {
          min_components
        } else {
          devs <- sapply(k_candidates, function(k) {
            tryCatch({
              Z <- pr$x[, seq_len(k), drop = FALSE]
              
              # Use Ridge regression on components for stability
              cv <- cv.glmnet(Z, y, family = "binomial", alpha = 0,
                             foldid = foldid, type.measure = "deviance",
                             standardize = FALSE, trace.it = 0)
              min(cv$cvm, na.rm = TRUE)
            }, error = function(e) {
              log_debug("PCR cvdev: k=%d failed: %s", k, e$message)
              Inf
            })
          })
          
          if (all(is.infinite(devs))) {
            log_warn("PCR cvdev: All models failed - using minimum components")
            min_components
          } else {
            k_candidates[which.min(devs)]
          }
        }
      }
    )
    
    # Final validation
    k <- max(min_components, min(k, max_k))
    
    log_debug("PCR %s selected %d components (range: %d-%d)", rule, k, min_components, max_k)
    
    # Enhanced result object
    result <- list(
      pr = pr,
      k = k,
      rule = rule,
      eigenvalues = eig,
      variance_explained = varexpl,
      selection_details = list(
        max_k = max_k,
        min_components = min_components,
        selection_time = as.numeric(pca_time)
      )
    )
    
    result
    
  }, error = function(e) {
    log_error("PCR component selection failed: %s", e$message)
    
    # Return minimal viable result
    list(
      pr = list(x = X, center = rep(0, ncol(X)), scale = rep(1, ncol(X)),
                rotation = diag(ncol(X))),
      k = 1,
      rule = rule,
      error = e$message
    )
  })
}

# =======================================================================
# REGISTER DEFAULT METHODS
# =======================================================================

# Register MLE
register_method(
  "MLE", 
  fit_mle, 
  predict_mle, 
  requires_cv = FALSE,
  description = "Maximum Likelihood Estimation (unpenalized logistic regression)"
)

# Register Ridge
register_method(
  "Ridge",
  function(X, y, foldid = NULL, ...) fit_glmnet_cv(X, y, alpha = 0, foldid = foldid),
  predict_glmnet_resp,
  requires_cv = TRUE,
  description = "Ridge regression (L2 penalty, alpha = 0)"
)

# Register LASSO  
register_method(
  "LASSO",
  function(X, y, foldid = NULL, ...) fit_glmnet_cv(X, y, alpha = 1, foldid = foldid),
  predict_glmnet_resp,
  requires_cv = TRUE,
  description = "LASSO (L1 penalty, alpha = 1)"
)

# Register Relaxed LASSO
register_method(
  "RelaxedLASSO",
  function(X, y, foldid = NULL, ...) fit_glmnet_cv(X, y, alpha = 1, foldid = foldid, relax = TRUE),
  predict_glmnet_resp,  
  requires_cv = TRUE,
  description = "Relaxed LASSO (two-stage procedure)"
)

# Print initialization message
cat("Enhanced Lohmann et al. (2023) replication framework loaded successfully.\n")
cat(sprintf("Configuration: %s\n", ifelse(file.exists("config/simulation_config.yaml"), "YAML loaded", "defaults")))
cat(sprintf("Logging: %s level to %s\n", 
            get_config(c("logging", "level"), "INFO"), 
            ifelse(get_config(c("logging", "file"), FALSE), "file + console", "console")))
cat(sprintf("Registered methods: %d\n", length(.method_registry)))
cat("Enhanced features: Memory management, Error handling, Performance profiling, Method registry\n")
cat("\nMain functions:\n")
cat("- load_config(): Load configuration from YAML\n")
cat("- register_method(): Add new methods to framework\n")
cat("- list_methods(): Show all registered methods\n")
cat("- All original functions with enhanced error handling and logging\n\n")
